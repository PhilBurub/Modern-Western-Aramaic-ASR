{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B32iysTxjnc_"
      },
      "outputs": [],
      "source": [
        "with open('req.txt', 'w', encoding='utf-8') as f:\n",
        "  f.write('''accelerate==0.15.0\n",
        "datasets==2.6.1\n",
        "huggingface-hub==0.11.1\n",
        "jiwer==2.5.1\n",
        "librosa==0.9.2\n",
        "espnet==202211\n",
        "espnet_model_zoo==0.1.7\n",
        "natsort==8.2.0\n",
        "numpy==1.23.5\n",
        "omegaconf==2.2.3\n",
        "pandas==1.5.2\n",
        "parallel-wavegan==0.5.5\n",
        "pyctcdecode==0.4.0\n",
        "soundfile==0.11.0\n",
        "torch==1.13.0\n",
        "torchaudio==0.13.0\n",
        "tqdm==4.64.1\n",
        "transformers==4.24.0\n",
        "wandb==0.13.4''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbvmVWEH7pVD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -r req.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05fRkdS75E67"
      },
      "outputs": [],
      "source": [
        "import omegaconf as oc\n",
        "import transformers as hft\n",
        "import librosa as lb\n",
        "from datasets import Dataset\n",
        "import jiwer\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import os\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import typing\n",
        "import dataclasses\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz-G1RTl7gRw"
      },
      "outputs": [],
      "source": [
        "def prRed(skk): print(\"\\033[91m {}\\033[00m\" .format(skk))\n",
        "def prGreen(skk): print(\"\\033[92m {}\\033[00m\" .format(skk))\n",
        "def prYellow(skk): print(\"\\033[93m{}\\033[00m\" .format(skk))\n",
        "def prLightPurple(skk): print(\"\\033[94m {}\\033[00m\" .format(skk))\n",
        "def prPurple(skk): print(\"\\033[95m {}\\033[00m\" .format(skk))\n",
        "def prCyan(skk): print(\"\\033[96m {}\\033[00m\" .format(skk))\n",
        "def prLightGray(skk): print(\"\\033[97m {}\\033[00m\" .format(skk))\n",
        "def prBlack(skk): print(\"\\033[98m {}\\033[00m\" .format(skk))\n",
        "\n",
        "def announce(announcement):\n",
        "    pad_length  = 5\n",
        "\n",
        "    print(f\"{'-' * pad_length} {announcement} {'-' * pad_length}\")\n",
        "\n",
        "def make_config(config):\n",
        "\n",
        "    # Overwrite config vars with anything supplied in the command line\n",
        "    config = oc.OmegaConf.merge(\n",
        "        oc.OmegaConf.load(config['--config']),\n",
        "        oc.OmegaConf.from_cli()\n",
        "    )\n",
        "\n",
        "    flat_args_long = pd.json_normalize(oc.OmegaConf.to_container(config), sep=\".\").melt(var_name='argument')\n",
        "    missing_args   = flat_args_long.query(\"value == '???'\")\n",
        "\n",
        "    assert len(missing_args) == 0, f\"\"\"\n",
        "\n",
        "    The following required arguments are missing:\n",
        "\n",
        "        {','.join(missing_args['argument'].to_list())}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    announce(\"Configuring environment\")\n",
        "\n",
        "    # Set environment variables\n",
        "    for key, value in config['env'].items():\n",
        "\n",
        "        if key == 'CUDA_VISIBLE_DEVICES':\n",
        "            # OmegaConf will coerce number-like values into integers\n",
        "            # but CUDA_VISIBLE_DEVICES should be a (comma-seperated) string\n",
        "            value = str(value)\n",
        "\n",
        "        os.environ[key] = value\n",
        "\n",
        "    if not 'wandb' in config.keys():\n",
        "\n",
        "        return config, None\n",
        "\n",
        "    else:\n",
        "        run = wandb.init(allow_val_change=True, settings=wandb.Settings(code_dir=\".\"), **config['wandb'])\n",
        "\n",
        "        if config.get(\"--run_name\"):\n",
        "            # Interpolate 'lr={tranargs[learning_rate]}' to 'lr=0.0001', where config['tranargs']['learning_rate'] = 0.0001\n",
        "            run.name = config[\"--run_name\"].format(**config)\n",
        "\n",
        "        # Log hyper-parameters not automatically tracked by wandb\n",
        "        untracked_args = flat_args_long[ ~flat_args_long.argument.str.contains(\"w2v2|trainargs|wandb|--\", regex=True) ]\n",
        "        # Convert to flat dict, e.g. { 'data.base_path' : '/path/to/the/data' }\n",
        "\n",
        "        untracked_args = dict([ (d['argument'], d['value']) for d in untracked_args.to_dict(orient='records') ])\n",
        "\n",
        "        wandb.config.update(untracked_args, allow_val_change=True)\n",
        "\n",
        "        config['trainargs']['report_to'] = \"wandb\"\n",
        "\n",
        "        return config, run\n",
        "\n",
        "def load_datasets(data_config, processor):\n",
        "\n",
        "    announce(\"Loading data ...\")\n",
        "\n",
        "    def _tsv2ds(tsv_file):\n",
        "\n",
        "        tsv_path = os.path.join(data_config['base_path'], data_config[tsv_file])\n",
        "\n",
        "        print(f\"Reading split from {tsv_path}\")\n",
        "\n",
        "        df = pd.read_csv(tsv_path, sep='\\t')\n",
        "\n",
        "        for c in ['path_col', 'text_col']:\n",
        "            col_name = data_config[c]\n",
        "\n",
        "            assert col_name in df.columns, f\"\\n\\n\\tDataset {tsv_path} is missing '{col_name}' column\\n\"\n",
        "\n",
        "        # Normalize column names\n",
        "        df = df.rename(columns = {\n",
        "            data_config['path_col'] : 'path',\n",
        "            data_config['text_col'] : 'text'\n",
        "        })\n",
        "\n",
        "        def _read_audio(path):\n",
        "            full_path = os.path.join(data_config['base_path'], path)\n",
        "\n",
        "            data, sr = lb.load(full_path, sr=None)\n",
        "\n",
        "            assert sr == 16_000\n",
        "\n",
        "            return data\n",
        "\n",
        "        df['audio'] = [ _read_audio(path) for path in tqdm(df['path'].to_list(), desc=\"Reading audio data\") ]\n",
        "\n",
        "        if 'subset_train' in data_config and tsv_file == 'train_tsv':\n",
        "\n",
        "            df = df.sample(frac=1, random_state=data_config['subset_train']['seed']).copy().reset_index(drop=True)\n",
        "            df = df[ df['audio'].apply(lambda s: len(s)/16_000).cumsum() <= (60 * data_config['subset_train']['mins']) ].copy().reset_index(drop=True)\n",
        "\n",
        "            prYellow(f\"Subsetted training data as specified: {data_config['subset_train']['mins']} minutes, random seed {data_config['subset_train']['seed']}. Rows kept: {len(df)}\")\n",
        "\n",
        "        # see files in subset\n",
        "        print(\"Files in training subset:\")\n",
        "        for f in df['path'].to_list():\n",
        "            print(f)\n",
        "\n",
        "        dataset = hfds.Dataset.from_pandas(df[['audio', 'text']])\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    datasets = hfds.DatasetDict({\n",
        "        'train' : _tsv2ds('train_tsv'),\n",
        "        'eval' : _tsv2ds('eval_tsv')\n",
        "    })\n",
        "\n",
        "    def _to_inputs_and_labels(batch):\n",
        "        batch[\"input_values\"] = processor(batch[\"audio\"], sampling_rate=16000).input_values[0]\n",
        "\n",
        "        batch[\"labels\"] = processor.tokenizer(batch[\"text\"]).input_ids\n",
        "\n",
        "        return batch\n",
        "\n",
        "    announce(\"Preparing input features and labels ...\")\n",
        "\n",
        "    datasets = datasets.map(_to_inputs_and_labels, remove_columns=['audio', 'text'])\n",
        "\n",
        "    return datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxzXRxD87jvP"
      },
      "outputs": [],
      "source": [
        "def configure_hf_w2v2_model(config):\n",
        "\n",
        "    print(f\"Loading {config['w2v2']['model']['pretrained_model_name_or_path']} model ...\")\n",
        "\n",
        "    # Set verbosity to error while loading models (skips warnings about loading a not-yet fine-tuned model)\n",
        "    hft.logging.set_verbosity_error()\n",
        "\n",
        "    # Re-use the vocab.json from the fine-tuned model instead of re-deriving it from the train/test data\n",
        "\n",
        "    # !wget https://huggingface.co/facebook/wav2vec2-large-960h/raw/main/vocab.json\n",
        "\n",
        "    if config['w2v2']['tok']['vocab_file'] is None:\n",
        "        # Load tokenizer from model if already fine-tuned\n",
        "        processor = hft.Wav2Vec2Processor.from_pretrained(config['w2v2']['model']['pretrained_model_name_or_path'])\n",
        "\n",
        "    else:\n",
        "        # Create a new processor (i.e. fine-tuning for the first time)\n",
        "        processor = hft.Wav2Vec2Processor(\n",
        "            tokenizer=hft.Wav2Vec2CTCTokenizer(**(config['w2v2']['tok'] or {})),\n",
        "            feature_extractor=hft.Wav2Vec2FeatureExtractor(**(config['w2v2']['fext'] or {})),\n",
        "            **(config['w2v2']['proc'] or {})\n",
        "        )\n",
        "\n",
        "    processor.save_pretrained(config['trainargs']['output_dir'])\n",
        "\n",
        "    model_config = hft.AutoConfig.from_pretrained(config['w2v2']['model']['pretrained_model_name_or_path'])\n",
        "\n",
        "    # set vocab size\n",
        "    f = open(config['w2v2']['tok']['vocab_file'])\n",
        "    vocab = json.load(f)\n",
        "    vocab_size = max(vocab.values()) + 1\n",
        "    model_config.vocab_size = vocab_size\n",
        "\n",
        "    config['w2v2']['model']['pad_token_id'] = processor.tokenizer.pad_token_id\n",
        "    config['w2v2']['model']['ctc_zero_infinity'] = True\n",
        "\n",
        "    model_config.update(config['w2v2']['model'])\n",
        "\n",
        "    model = hft.Wav2Vec2ForCTC.from_pretrained(\n",
        "        config['w2v2']['model']['pretrained_model_name_or_path'],\n",
        "        config=model_config\n",
        "    )\n",
        "\n",
        "    model.freeze_feature_encoder()\n",
        "\n",
        "    return model, processor\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "\n",
        "    processor: hft.Wav2Vec2Processor\n",
        "    padding: typing.Union[bool, str] = True\n",
        "\n",
        "    def __call__(self, features: typing.List[typing.Dict[str, typing.Union[typing.List[int], torch.Tensor]]]) -> typing.Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        labels_batch = self.processor.tokenizer.pad(\n",
        "            label_features,\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "class MetricsComputer:\n",
        "\n",
        "    def __init__(self, config, processor):\n",
        "\n",
        "        self.processor = processor\n",
        "        self.report_to = config['trainargs']['report_to']\n",
        "\n",
        "        decode_method = config['w2v2']['decode']['method']\n",
        "\n",
        "        assert decode_method in ['greedy', 'beam_search'], f\"\\n\\tError: Unrecognized decoding method '{decode_method}'\"\n",
        "\n",
        "        if decode_method == 'greedy':\n",
        "\n",
        "            self.decoder = self.greedy_decoder\n",
        "\n",
        "        elif decode_method == 'beam_search':\n",
        "\n",
        "            from torchaudio.models.decoder import ctc_decoder\n",
        "            from functools import partial\n",
        "\n",
        "            _decoder = ctc_decoder(\n",
        "                lexicon=None,\n",
        "                tokens=list(processor.tokenizer.get_vocab().keys()),\n",
        "                blank_token=processor.tokenizer.pad_token,\n",
        "                sil_token=processor.tokenizer.word_delimiter_token,\n",
        "                unk_word=processor.tokenizer.unk_token,\n",
        "                **config['w2v2']['decode']['args']\n",
        "            )\n",
        "\n",
        "            self.decoder = partial(self.beam_search_decoder, decoder=_decoder)\n",
        "\n",
        "    def __call__(self, pred):\n",
        "\n",
        "        labels = self.get_labels(pred)\n",
        "        preds  = self.decoder(pred)\n",
        "\n",
        "        wer, cer = self.compute_metrics(labels, preds)\n",
        "\n",
        "        return { \"wer\" : wer, \"cer\" : cer }\n",
        "\n",
        "    def get_labels(self, pred):\n",
        "        # Replace data collator padding with tokenizer's padding\n",
        "        pred.label_ids[pred.label_ids == -100] = self.processor.tokenizer.pad_token_id\n",
        "        # Retrieve labels as characters, e.g. 'hello', from label_ids, e.g. [5, 3, 10, 10, 2] (where 5 = 'h')\n",
        "        label_str = self.processor.tokenizer.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "        return label_str\n",
        "\n",
        "    def beam_search_decoder(self, pred, decoder):\n",
        "\n",
        "        pred_logits = torch.tensor(pred.predictions, dtype=torch.float32)\n",
        "\n",
        "        from tqdm import tqdm\n",
        "        from joblib import Parallel, delayed\n",
        "\n",
        "        def logits_to_preds(logits):\n",
        "            # unsqueeze to make logits to shape (B=1, T, V) expected by decode\n",
        "            # instead of just (T, V), where B = batch, T = time steps, V = vocab size\n",
        "            hypotheses = decoder(logits.unsqueeze(0))\n",
        "\n",
        "            # Subset to get hypotheses for first example (of batch size 1)\n",
        "            hypotheses = hypotheses[0]\n",
        "\n",
        "            # Return top hypothesis as a string\n",
        "            return self.processor.decode(hypotheses[0].tokens)\n",
        "\n",
        "        # Decode in parallel\n",
        "        pred_str = Parallel(n_jobs=-1, verbose=0, prefer=\"threads\")(delayed(logits_to_preds)(l) for l in tqdm(pred_logits, desc=\"Running beam search decoding ...\"))\n",
        "\n",
        "        return pred_str\n",
        "\n",
        "    def greedy_decoder(self, pred):\n",
        "\n",
        "        pred_logits = pred.predictions\n",
        "        pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "        pred_str = self.processor.batch_decode(pred_ids)\n",
        "\n",
        "        return pred_str\n",
        "\n",
        "    def compute_metrics(self, labels, preds):\n",
        "\n",
        "        scoring_df = pd.DataFrame({\"Reference\" : labels, \"Prediction\"  : preds})\n",
        "\n",
        "        if self.report_to == 'wandb':\n",
        "            wandb.log({ \"asr_out\": wandb.Table(data=scoring_df) })\n",
        "\n",
        "        # Print two newlines first to separate table from progress bar\n",
        "        print(\"\\n\\n\")\n",
        "        print(scoring_df)\n",
        "\n",
        "        wer = jiwer.wer(labels, preds)\n",
        "        cer = jiwer.cer(labels, preds)\n",
        "\n",
        "        return wer, cer\n",
        "\n",
        "# Adapted from https://discuss.huggingface.co/t/weights-biases-supporting-wave2vec2-finetuning/4839/4\n",
        "def get_flat_linear_schedule_with_warmup(optimizer, num_warmup_steps:int, num_training_steps:int, last_epoch:int =-1, lr_warmup_pc=0.1, lr_const_pc=0.4):\n",
        "\n",
        "    def lr_lambda(current_step):\n",
        "        constant_steps = int(num_training_steps * lr_const_pc)\n",
        "        warmup_steps = int(num_training_steps * lr_warmup_pc)\n",
        "\n",
        "        if current_step < warmup_steps:\n",
        "            return float(current_step) / float(max(1, warmup_steps))\n",
        "        elif current_step < warmup_steps+constant_steps:\n",
        "            return 1\n",
        "        else:\n",
        "            return max(\n",
        "                0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - (warmup_steps+constant_steps)))\n",
        "            )\n",
        "\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)\n",
        "\n",
        "def get_flat_cheduler(name = None, optimizer = None, num_warmup_steps = None, num_training_steps = None):\n",
        "    return get_flat_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
        "\n",
        "class ReplicationTrainer(hft.Trainer):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def create_flat_scheduler(self, num_training_steps: int):\n",
        "        self.lr_scheduler = get_flat_cheduler(optimizer = self.optimizer,\n",
        "                                              num_training_steps=num_training_steps)\n",
        "\n",
        "    def create_optimizer_and_scheduler(self, num_training_steps):\n",
        "        self.create_optimizer()\n",
        "        self.create_flat_scheduler(num_training_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "tz5sC7c_D9CV",
        "outputId": "6cfe8347-dbe4-423b-9a25-531532fd618a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Configuring environment -----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpburub\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "wandb version 0.17.0 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240516_133040-nzhpl8br</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/pburub/ft_synt/runs/nzhpl8br\" target=\"_blank\">run_1</a></strong> to <a href=\"https://wandb.ai/pburub/ft_synt\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "config, wandb_run = make_config({'--config': '/content/drive/MyDrive/configs/eval_wo_synt.yaml'})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# devset_path = os.path.join(config['data'].base_path, config['data'].train_tsv)\n",
        "testset_path = os.path.join(config['data'].base_path, config['data'].eval_tsv)\n",
        "valset_path = os.path.join(config['data'].base_path, config['data'].dev_tsv)\n",
        "\n",
        "# print(f\"Development set: {devset_path}\")\n",
        "print(f\"Test set: {testset_path}\")\n",
        "print(f\"Val set: {valset_path}\")\n",
        "\n",
        "# dev_ds = pd.read_csv(devset_path, sep = '\\t')\n",
        "test_ds = pd.read_csv(testset_path, sep = '\\t')\n",
        "val_ds = pd.read_csv(valset_path, sep = '\\t')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhSmQsh35WMv",
        "outputId": "d87bf530-de21-4fb0-96f9-97fae2b7222b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test set: /content/drive/MyDrive/dataset/test.tsv\n",
            "Val set: /content/drive/MyDrive/dataset/dev.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMu1qdG4MtzY",
        "outputId": "088aa028-4bd9-4f24-cb3c-b304c6220d06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading audio data: 100%|██████████| 1740/1740 [00:03<00:00, 443.23it/s]\n",
            "Reading audio data: 100%|██████████| 300/300 [01:08<00:00,  4.38it/s]\n"
          ]
        }
      ],
      "source": [
        "def _read_audio(path):\n",
        "    full_path = os.path.join(config['data'].base_path, path)\n",
        "\n",
        "    data, sr = sf.read(full_path)\n",
        "\n",
        "    assert sr == 16_000\n",
        "\n",
        "    return data\n",
        "\n",
        "# dev_ds['audio'] = [ _read_audio(path) for path in tqdm(dev_ds['path'].to_list(), desc='Reading audio data') ]\n",
        "test_ds['audio'] = [ _read_audio(path) for path in tqdm(test_ds['path'].to_list(), desc='Reading audio data') ]\n",
        "val_ds['audio'] = [ _read_audio(path) for path in tqdm(val_ds['path'].to_list(), desc='Reading audio data') ]\n",
        "\n",
        "# dev_ds = Dataset.from_pandas(dev_ds[['audio', 'text']])\n",
        "test_ds = Dataset.from_pandas(test_ds[['audio', 'text']])\n",
        "val_ds = Dataset.from_pandas(val_ds[['audio', 'text']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVNlGEyUMBY1",
        "outputId": "5613a8c2-9aeb-41f2-f9c5-ff81323ab119"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Configuring model and reading data -----\n",
            "Loading /content/drive/MyDrive/model_ft_2/checkpoint-15000 model ...\n"
          ]
        }
      ],
      "source": [
        "announce('Configuring model and reading data')\n",
        "\n",
        "model, processor = configure_hf_w2v2_model(config)\n",
        "model = model.eval().cuda()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchaudio.models.decoder import ctc_decoder\n",
        "_decoder = ctc_decoder(\n",
        "    beam_size=10,\n",
        "    lexicon='/content/drive/MyDrive/lm/lexicon.txt',\n",
        "    lm='/content/drive/MyDrive/lm/model.bin',\n",
        "\n",
        "    # lexicon=None,\n",
        "    tokens=list(processor.tokenizer.get_vocab().keys()),\n",
        "    blank_token=processor.tokenizer.pad_token,\n",
        "    sil_token=processor.tokenizer.word_delimiter_token,\n",
        "    unk_word=processor.tokenizer.unk_token,\n",
        "    #**config['w2v2']['decode']['args']\n",
        ")\n",
        "\n",
        "def beam_search_decoder(pred, decoder, processor):\n",
        "      pred_logits = torch.tensor(pred, dtype=torch.float32)\n",
        "\n",
        "      from tqdm import tqdm\n",
        "      from joblib import Parallel, delayed\n",
        "\n",
        "      def logits_to_preds(logits):\n",
        "          # unsqueeze to make logits to shape (B=1, T, V) expected by decode\n",
        "          # instead of just (T, V), where B = batch, T = time steps, V = vocab size\n",
        "          hypotheses = decoder(logits.unsqueeze(0))\n",
        "\n",
        "          # Subset to get hypotheses for first example (of batch size 1)\n",
        "          hypotheses = hypotheses[0]\n",
        "\n",
        "          # Return top hypothesis as a string\n",
        "          return processor.decode(hypotheses[0].tokens)\n",
        "\n",
        "      # Decode in parallel\n",
        "      pred_str = Parallel(n_jobs=-1, verbose=0, prefer=\"threads\")(delayed(logits_to_preds)(l) for l in pred_logits)\n",
        "\n",
        "      return pred_str"
      ],
      "metadata": {
        "id": "tg4hfekZWhal"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8Pl0G0eMx7O"
      },
      "outputs": [],
      "source": [
        "announce('Evaluating model')\n",
        "\n",
        "def evaluate(batch):\n",
        "    inputs = processor(batch['audio'], sampling_rate=16_000, return_tensors='pt', padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs.input_values.to('cuda'), attention_mask=inputs.attention_mask.to('cuda')).logits\n",
        "\n",
        "    batch['transcription'] = beam_search_decoder(pred=logits.cpu(), decoder=_decoder, processor=processor)\n",
        "    # pred_ids = np.argmax(logits.cpu(), axis=-1)\n",
        "    # batch['transcription'] = processor.batch_decode(pred_ids)\n",
        "\n",
        "    return batch\n",
        "\n",
        "# dev_ds = dev_ds.map(evaluate, batched=True, batch_size=8)\n",
        "test_ds = test_ds.map(evaluate, batched=True, batch_size=8)\n",
        "# val_ds = val_ds.map(evaluate, batched=True, batch_size=8)\n",
        "\n",
        "# wer_dev = round(jiwer.wer(dev_ds['text'], dev_ds['transcription']), 4)\n",
        "# cer_dev = round(jiwer.cer(dev_ds['text'], dev_ds['transcription']), 4)\n",
        "\n",
        "# wer_val = round(jiwer.wer(val_ds['text'], val_ds['transcription']), 4)\n",
        "# cer_val = round(jiwer.cer(val_ds['text'], val_ds['transcription']), 4)\n",
        "\n",
        "wer_test = round(jiwer.wer(test_ds['text'], test_ds['transcription']), 4)\n",
        "cer_test = round(jiwer.cer(test_ds['text'], test_ds['transcription']), 4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxRq0VSm8ocG"
      },
      "outputs": [],
      "source": [
        "# announce('Results on development data')\n",
        "\n",
        "# print(f\"WER: {wer_dev} CER: {cer_dev}\")\n",
        "\n",
        "announce('Results on test data')\n",
        "\n",
        "print(f\"WER: {wer_test} CER: {cer_test}\")\n",
        "\n",
        "# announce('Results on val data')\n",
        "\n",
        "# print(f\"WER: {wer_val} CER: {cer_val}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = {}\n",
        "for beam_size in range(5, 55, 5):\n",
        "  _decoder = ctc_decoder(\n",
        "    beam_size=beam_size,\n",
        "    # lexicon='/content/drive/MyDrive/lm/lexicon.txt',\n",
        "    # lm='/content/drive/MyDrive/lm/model.bin',\n",
        "\n",
        "    lexicon=None,\n",
        "    tokens=list(processor.tokenizer.get_vocab().keys()),\n",
        "    blank_token=processor.tokenizer.pad_token,\n",
        "    sil_token=processor.tokenizer.word_delimiter_token,\n",
        "    unk_word=processor.tokenizer.unk_token,\n",
        "    #**config['w2v2']['decode']['args']\n",
        "  )\n",
        "  val_ds = val_ds.map(evaluate, batched=True, batch_size=8)\n",
        "  wer_val = round(jiwer.wer(val_ds['text'], val_ds['transcription']), 4)\n",
        "  cer_val = round(jiwer.cer(val_ds['text'], val_ds['transcription']), 4)\n",
        "  res[beam_size] = {'wer': wer_val, 'cer': cer_val}\n",
        "  print(f\"WER: {wer_val} CER: {cer_val}, {beam_size} beams\")"
      ],
      "metadata": {
        "id": "zDVEf_77rqxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds.to_pandas()[['text', 'transcription']].to_csv('greedy.csv')"
      ],
      "metadata": {
        "id": "SRL-soKA8waX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0lm5SHupi29H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}