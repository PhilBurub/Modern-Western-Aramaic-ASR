{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "with open('req.txt', 'w', encoding='utf-8') as f:\n",
        "  f.write('''accelerate==0.15.0\n",
        "datasets==2.18.0\n",
        "huggingface-hub==0.22.2\n",
        "jiwer==2.5.1\n",
        "librosa==0.9.2\n",
        "espnet==202211\n",
        "espnet_model_zoo==0.1.7\n",
        "natsort==8.2.0\n",
        "numpy==1.23.5\n",
        "omegaconf==2.2.3\n",
        "pandas==1.5.2\n",
        "parallel-wavegan==0.5.5\n",
        "pyctcdecode==0.4.0\n",
        "soundfile==0.11.0\n",
        "torch==1.13.0\n",
        "torchaudio==0.13.0\n",
        "tqdm==4.64.1\n",
        "transformers==4.24.0\n",
        "wandb==0.13.4''')"
      ],
      "metadata": {
        "id": "B32iysTxjnc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TbvmVWEH7pVD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -r req.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05fRkdS75E67"
      },
      "outputs": [],
      "source": [
        "import omegaconf as oc\n",
        "import transformers as hft\n",
        "import librosa as lb\n",
        "import datasets as hfds\n",
        "import jiwer\n",
        "import pandas as pd\n",
        "import soundfile as sf\n",
        "import os\n",
        "import wandb\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import typing\n",
        "import dataclasses\n",
        "import json\n",
        "from datasets import Dataset\n",
        "from pathlib import Path\n",
        "from transformers import set_seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pz-G1RTl7gRw"
      },
      "outputs": [],
      "source": [
        "def prRed(skk): print(\"\\033[91m {}\\033[00m\" .format(skk))\n",
        "def prGreen(skk): print(\"\\033[92m {}\\033[00m\" .format(skk))\n",
        "def prYellow(skk): print(\"\\033[93m{}\\033[00m\" .format(skk))\n",
        "def prLightPurple(skk): print(\"\\033[94m {}\\033[00m\" .format(skk))\n",
        "def prPurple(skk): print(\"\\033[95m {}\\033[00m\" .format(skk))\n",
        "def prCyan(skk): print(\"\\033[96m {}\\033[00m\" .format(skk))\n",
        "def prLightGray(skk): print(\"\\033[97m {}\\033[00m\" .format(skk))\n",
        "def prBlack(skk): print(\"\\033[98m {}\\033[00m\" .format(skk))\n",
        "\n",
        "def announce(announcement):\n",
        "    pad_length  = 5\n",
        "\n",
        "    print(f\"{'-' * pad_length} {announcement} {'-' * pad_length}\")\n",
        "\n",
        "def make_config(config):\n",
        "\n",
        "    # Overwrite config vars with anything supplied in the command line\n",
        "    config = oc.OmegaConf.merge(\n",
        "        oc.OmegaConf.load(config['--config']),\n",
        "        oc.OmegaConf.from_cli()\n",
        "    )\n",
        "\n",
        "    flat_args_long = pd.json_normalize(oc.OmegaConf.to_container(config), sep=\".\").melt(var_name='argument')\n",
        "    missing_args   = flat_args_long.query(\"value == '???'\")\n",
        "\n",
        "    assert len(missing_args) == 0, f\"\"\"\n",
        "\n",
        "    The following required arguments are missing:\n",
        "\n",
        "        {','.join(missing_args['argument'].to_list())}\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    announce(\"Configuring environment\")\n",
        "\n",
        "    # Set environment variables\n",
        "    for key, value in config['env'].items():\n",
        "\n",
        "        if key == 'CUDA_VISIBLE_DEVICES':\n",
        "            # OmegaConf will coerce number-like values into integers\n",
        "            # but CUDA_VISIBLE_DEVICES should be a (comma-seperated) string\n",
        "            value = str(value)\n",
        "\n",
        "        os.environ[key] = value\n",
        "\n",
        "    if not 'wandb' in config.keys():\n",
        "\n",
        "        return config, None\n",
        "\n",
        "    else:\n",
        "        run = wandb.init(allow_val_change=True, settings=wandb.Settings(code_dir=\".\"), **config['wandb'])\n",
        "\n",
        "        if config.get(\"--run_name\"):\n",
        "            # Interpolate 'lr={tranargs[learning_rate]}' to 'lr=0.0001', where config['tranargs']['learning_rate'] = 0.0001\n",
        "            run.name = config[\"--run_name\"].format(**config)\n",
        "\n",
        "        # Log hyper-parameters not automatically tracked by wandb\n",
        "        untracked_args = flat_args_long[ ~flat_args_long.argument.str.contains(\"w2v2|trainargs|wandb|--\", regex=True) ]\n",
        "        # Convert to flat dict, e.g. { 'data.base_path' : '/path/to/the/data' }\n",
        "        untracked_args = dict([ (d['argument'], d['value']) for d in untracked_args.to_dict(orient='records') ])\n",
        "\n",
        "        wandb.config.update(untracked_args, allow_val_change=True)\n",
        "\n",
        "        config['trainargs']['report_to'] = \"wandb\"\n",
        "\n",
        "        return config, run\n",
        "\n",
        "def load_datasets(data_config, processor):\n",
        "\n",
        "    announce(\"Loading data ...\")\n",
        "\n",
        "    def _tsv2ds(tsv_file):\n",
        "\n",
        "        tsv_path = os.path.join(data_config['base_path'], data_config[tsv_file])\n",
        "\n",
        "        print(f\"Reading split from {tsv_path}\")\n",
        "\n",
        "        df = pd.read_csv(tsv_path, sep='\\t')\n",
        "\n",
        "        for c in ['path_col', 'text_col']:\n",
        "            col_name = data_config[c]\n",
        "\n",
        "            assert col_name in df.columns, f\"\\n\\n\\tDataset {tsv_path} is missing '{col_name}' column\\n\"\n",
        "\n",
        "        # Normalize column names\n",
        "        df = df.rename(columns = {\n",
        "            data_config['path_col'] : 'path',\n",
        "            data_config['text_col'] : 'text'\n",
        "        })\n",
        "\n",
        "        def _read_audio(path):\n",
        "            full_path = os.path.join(data_config['base_path'], path)\n",
        "\n",
        "            data, sr = lb.load(full_path, sr=None)\n",
        "\n",
        "            assert sr == 16_000\n",
        "\n",
        "            return data\n",
        "\n",
        "        df['audio'] = [ _read_audio(path) for path in tqdm(df['path'].to_list(), desc=\"Reading audio data\") ]\n",
        "\n",
        "        if 'subset_train' in data_config and tsv_file == 'train_tsv':\n",
        "\n",
        "            df = df.sample(frac=1, random_state=data_config['subset_train']['seed']).copy().reset_index(drop=True)\n",
        "            df = df[ df['audio'].apply(lambda s: len(s)/16_000).cumsum() <= (60 * data_config['subset_train']['mins']) ].copy().reset_index(drop=True)\n",
        "\n",
        "            prYellow(f\"Subsetted training data as specified: {data_config['subset_train']['mins']} minutes, random seed {data_config['subset_train']['seed']}. Rows kept: {len(df)}\")\n",
        "\n",
        "        # see files in subset\n",
        "        print(\"Files in training subset:\")\n",
        "        for f in df['path'].to_list():\n",
        "            print(f)\n",
        "\n",
        "        dataset = hfds.Dataset.from_pandas(df[['audio', 'text']])\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    datasets = hfds.DatasetDict({\n",
        "        'train' : _tsv2ds('train_tsv'),\n",
        "        'eval' : _tsv2ds('eval_tsv')\n",
        "    })\n",
        "\n",
        "    def _to_inputs_and_labels(batch):\n",
        "        batch[\"input_values\"] = processor(batch[\"audio\"], sampling_rate=16000).input_values[0]\n",
        "\n",
        "        batch[\"labels\"] = processor.tokenizer(batch[\"text\"]).input_ids\n",
        "\n",
        "        return batch\n",
        "\n",
        "    announce(\"Preparing input features and labels ...\")\n",
        "\n",
        "    datasets = datasets.map(_to_inputs_and_labels, remove_columns=['audio', 'text'])\n",
        "\n",
        "    return datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxzXRxD87jvP"
      },
      "outputs": [],
      "source": [
        "def configure_hf_w2v2_model(config):\n",
        "\n",
        "    print(f\"Loading {config['w2v2']['model']['pretrained_model_name_or_path']} model ...\")\n",
        "\n",
        "    # Set verbosity to error while loading models (skips warnings about loading a not-yet fine-tuned model)\n",
        "    hft.logging.set_verbosity_error()\n",
        "\n",
        "    # Re-use the vocab.json from the fine-tuned model instead of re-deriving it from the train/test data\n",
        "\n",
        "    # !wget https://huggingface.co/facebook/wav2vec2-large-960h/raw/main/vocab.json\n",
        "\n",
        "    if config['w2v2']['tok']['vocab_file'] is None:\n",
        "        # Load tokenizer from model if already fine-tuned\n",
        "        processor = hft.Wav2Vec2Processor.from_pretrained(config['w2v2']['model']['pretrained_model_name_or_path'])\n",
        "\n",
        "    else:\n",
        "        # Create a new processor (i.e. fine-tuning for the first time)\n",
        "        processor = hft.Wav2Vec2Processor(\n",
        "            tokenizer=hft.Wav2Vec2CTCTokenizer(**(config['w2v2']['tok'] or {})),\n",
        "            feature_extractor=hft.Wav2Vec2FeatureExtractor(**(config['w2v2']['fext'] or {})),\n",
        "            **(config['w2v2']['proc'] or {})\n",
        "        )\n",
        "\n",
        "    processor.save_pretrained(config['trainargs']['output_dir'])\n",
        "\n",
        "    model_config = hft.AutoConfig.from_pretrained(config['w2v2']['model']['pretrained_model_name_or_path'])\n",
        "\n",
        "    # set vocab size\n",
        "    f = open(config['w2v2']['tok']['vocab_file'])\n",
        "    vocab = json.load(f)\n",
        "    vocab_size = max(vocab.values()) + 1\n",
        "    model_config.vocab_size = vocab_size\n",
        "\n",
        "    config['w2v2']['model']['pad_token_id'] = processor.tokenizer.pad_token_id\n",
        "    config['w2v2']['model']['ctc_zero_infinity'] = True\n",
        "\n",
        "    model_config.update(config['w2v2']['model'])\n",
        "\n",
        "    model = hft.Wav2Vec2ForCTC.from_pretrained(\n",
        "        config['w2v2']['model']['pretrained_model_name_or_path'],\n",
        "        config=model_config\n",
        "    )\n",
        "\n",
        "    model.freeze_feature_encoder()\n",
        "\n",
        "    return model, processor\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class DataCollatorCTCWithPadding:\n",
        "\n",
        "    processor: hft.Wav2Vec2Processor\n",
        "    padding: typing.Union[bool, str] = True\n",
        "\n",
        "    def __call__(self, features: typing.List[typing.Dict[str, typing.Union[typing.List[int], torch.Tensor]]]) -> typing.Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lenghts and need\n",
        "        # different padding methods\n",
        "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "\n",
        "        batch = self.processor.pad(\n",
        "            input_features,\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        labels_batch = self.processor.tokenizer.pad(\n",
        "            label_features,\n",
        "            padding=self.padding,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch\n",
        "\n",
        "class MetricsComputer:\n",
        "\n",
        "    def __init__(self, config, processor):\n",
        "\n",
        "        self.processor = processor\n",
        "        self.report_to = config['trainargs']['report_to']\n",
        "\n",
        "        decode_method = config['w2v2']['decode']['method']\n",
        "\n",
        "        assert decode_method in ['greedy', 'beam_search'], f\"\\n\\tError: Unrecognized decoding method '{decode_method}'\"\n",
        "\n",
        "        if decode_method == 'greedy':\n",
        "\n",
        "            self.decoder = self.greedy_decoder\n",
        "\n",
        "        elif decode_method == 'beam_search':\n",
        "\n",
        "            from torchaudio.models.decoder import ctc_decoder\n",
        "            from functools import partial\n",
        "\n",
        "            _decoder = ctc_decoder(\n",
        "                lexicon=None,\n",
        "                tokens=list(processor.tokenizer.get_vocab().keys()),\n",
        "                blank_token=processor.tokenizer.pad_token,\n",
        "                sil_token=processor.tokenizer.word_delimiter_token,\n",
        "                unk_word=processor.tokenizer.unk_token,\n",
        "                **config['w2v2']['decode']['args']\n",
        "            )\n",
        "\n",
        "            self.decoder = partial(self.beam_search_decoder, decoder=_decoder)\n",
        "\n",
        "    def __call__(self, pred):\n",
        "\n",
        "        labels = self.get_labels(pred)\n",
        "        preds  = self.decoder(pred)\n",
        "\n",
        "        wer, cer = self.compute_metrics(labels, preds)\n",
        "\n",
        "        return { \"wer\" : wer, \"cer\" : cer }\n",
        "\n",
        "    def get_labels(self, pred):\n",
        "        # Replace data collator padding with tokenizer's padding\n",
        "        pred.label_ids[pred.label_ids == -100] = self.processor.tokenizer.pad_token_id\n",
        "        # Retrieve labels as characters, e.g. 'hello', from label_ids, e.g. [5, 3, 10, 10, 2] (where 5 = 'h')\n",
        "        label_str = self.processor.tokenizer.batch_decode(pred.label_ids, group_tokens=False)\n",
        "\n",
        "        return label_str\n",
        "\n",
        "    def beam_search_decoder(self, pred, decoder):\n",
        "\n",
        "        pred_logits = torch.tensor(pred.predictions, dtype=torch.float32)\n",
        "\n",
        "        from tqdm import tqdm\n",
        "        from joblib import Parallel, delayed\n",
        "\n",
        "        def logits_to_preds(logits):\n",
        "            # unsqueeze to make logits to shape (B=1, T, V) expected by decode\n",
        "            # instead of just (T, V), where B = batch, T = time steps, V = vocab size\n",
        "            hypotheses = decoder(logits.unsqueeze(0))\n",
        "\n",
        "            # Subset to get hypotheses for first example (of batch size 1)\n",
        "            hypotheses = hypotheses[0]\n",
        "\n",
        "            # Return top hypothesis as a string\n",
        "            return self.processor.decode(hypotheses[0].tokens)\n",
        "\n",
        "        # Decode in parallel\n",
        "        pred_str = Parallel(n_jobs=-1, verbose=0, prefer=\"threads\")(delayed(logits_to_preds)(l) for l in tqdm(pred_logits, desc=\"Running beam search decoding ...\"))\n",
        "\n",
        "        return pred_str\n",
        "\n",
        "    def greedy_decoder(self, pred):\n",
        "\n",
        "        pred_logits = pred.predictions\n",
        "        pred_ids = np.argmax(pred_logits, axis=-1)\n",
        "        pred_str = self.processor.batch_decode(pred_ids)\n",
        "\n",
        "        return pred_str\n",
        "\n",
        "    def compute_metrics(self, labels, preds):\n",
        "\n",
        "        scoring_df = pd.DataFrame({\"Reference\" : labels, \"Prediction\"  : preds})\n",
        "\n",
        "        if self.report_to == 'wandb':\n",
        "            wandb.log({ \"asr_out\": wandb.Table(data=scoring_df) })\n",
        "\n",
        "        # Print two newlines first to separate table from progress bar\n",
        "        print(\"\\n\\n\")\n",
        "        print(scoring_df)\n",
        "\n",
        "        wer = jiwer.wer(labels, preds)\n",
        "        cer = jiwer.cer(labels, preds)\n",
        "\n",
        "        return wer, cer\n",
        "\n",
        "# Adapted from https://discuss.huggingface.co/t/weights-biases-supporting-wave2vec2-finetuning/4839/4\n",
        "def get_flat_linear_schedule_with_warmup(optimizer, num_warmup_steps:int, num_training_steps:int, last_epoch:int =-1, lr_warmup_pc=0.1, lr_const_pc=0.4):\n",
        "\n",
        "    def lr_lambda(current_step):\n",
        "        constant_steps = int(num_training_steps * lr_const_pc)\n",
        "        warmup_steps = int(num_training_steps * lr_warmup_pc)\n",
        "\n",
        "        if current_step < warmup_steps:\n",
        "            return float(current_step) / float(max(1, warmup_steps))\n",
        "        elif current_step < warmup_steps+constant_steps:\n",
        "            return 1\n",
        "        else:\n",
        "            return max(\n",
        "                0.0, float(num_training_steps - current_step) / float(max(1, num_training_steps - (warmup_steps+constant_steps)))\n",
        "            )\n",
        "\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch)\n",
        "\n",
        "def get_flat_cheduler(name = None, optimizer = None, num_warmup_steps = None, num_training_steps = None):\n",
        "    return get_flat_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps, num_training_steps=num_training_steps)\n",
        "\n",
        "class ReplicationTrainer(hft.Trainer):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def create_flat_scheduler(self, num_training_steps: int):\n",
        "        self.lr_scheduler = get_flat_cheduler(optimizer = self.optimizer,\n",
        "                                              num_training_steps=num_training_steps)\n",
        "\n",
        "    def create_optimizer_and_scheduler(self, num_training_steps):\n",
        "        self.create_optimizer()\n",
        "        self.create_flat_scheduler(num_training_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config, wandb_run = make_config({'--config': '/content/drive/MyDrive/configs/generate_config.yaml'})\n",
        "\n",
        "announce('Configuring model and reading data')\n",
        "\n",
        "model, processor = configure_hf_w2v2_model(config)\n",
        "model = model.eval().cuda()\n",
        "\n",
        "devset_path = os.path.join(config['data'].base_path, config['data'].transcribe_tsv)\n",
        "\n",
        "print(f\"Data to transcribe: {devset_path}\")"
      ],
      "metadata": {
        "id": "TL7sph0T5SAU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "a7f014c6-dce3-403c-ad92-44f6f403ebed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Configuring environment -----\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mpburub\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "wandb version 0.17.0 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.13.4"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20240511_072218-wx25hawu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/pburub/transcription/runs/wx25hawu\" target=\"_blank\">run_1</a></strong> to <a href=\"https://wandb.ai/pburub/transcription\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Configuring model and reading data -----\n",
            "Loading /content/drive/MyDrive/model_ft/checkpoint-60000 model ...\n",
            "Data to transcribe: /content/drive/MyDrive/copt/train.tsv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_ds = pd.read_csv(devset_path, sep = '\\t')"
      ],
      "metadata": {
        "id": "vY-37e_ZzfKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _read_audio(path):\n",
        "    full_path = os.path.join(config['data'].base_path, path)\n",
        "\n",
        "    data, sr = sf.read(full_path)\n",
        "\n",
        "    assert sr == 16_000\n",
        "\n",
        "    return data\n",
        "\n",
        "dev_ds['audio'] = [ _read_audio(path) for path in tqdm(dev_ds['path'].to_list(), desc='Reading audio data') ]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYMsTi0iNdAr",
        "outputId": "738025ca-67cb-4c28-f3e0-1083ed8e2429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reading audio data: 100%|██████████| 3000/3000 [20:44<00:00,  2.41it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dev_ds = Dataset.from_pandas(dev_ds[['audio', 'path']])"
      ],
      "metadata": {
        "id": "unx-r_sUc5ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "announce('Evaluating model')\n",
        "\n",
        "def evaluate(batch):\n",
        "    inputs = processor(batch['audio'], sampling_rate=16_000, return_tensors='pt', padding=True)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(inputs.input_values.to('cuda'), attention_mask=inputs.attention_mask.to('cuda')).logits\n",
        "\n",
        "    pred_ids = np.argmax(logits.cpu(), axis=-1)\n",
        "    batch['transcription'] = processor.batch_decode(pred_ids)\n",
        "\n",
        "    return batch\n",
        "\n",
        "dev_ds = dev_ds.map(evaluate, batched=True, batch_size=2)\n",
        "dev_ds = dev_ds.to_pandas()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "aedebec22c0341138809cf7954a8cd56",
            "9a476f1f88be4cf1b2a66215c74fc4e6",
            "b2c45cc22ce1401b9c5b9b091e41b2c8",
            "c3688bea84e64523ba29a40cb652988d",
            "779596defbe14a67b6590aa0d0a8f7c6",
            "86cff63d47724b68aa1f5f48aad4cf5a",
            "06c418b4f65448c5941fa97731a1658e",
            "aa513ff3150a432fbf7da45df0281974",
            "6f9fbefaa5a04fdaae4a16606a31c989",
            "7763c45f0d8f4fe695d65a99d2a76223",
            "e817ac49138247aba853f9158d9e482c"
          ]
        },
        "id": "zISK1eQWNkGO",
        "outputId": "649148d0-7fdd-41d4-8919-bb535636dedd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Evaluating model -----\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/3000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aedebec22c0341138809cf7954a8cd56"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "announce('Transcribing')\n",
        "os.makedirs('./data/transcriptions/', exist_ok=True)\n",
        "dev_ds[['path', 'transcription']].to_csv('/content/drive/MyDrive/copt/file.tsv', sep = '\\t')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBKH0caRNVFp",
        "outputId": "b892d9f7-07dd-4e56-d8ee-4c4c5fba271c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----- Transcribing -----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kFCAmQW8OPLe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "aedebec22c0341138809cf7954a8cd56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9a476f1f88be4cf1b2a66215c74fc4e6",
              "IPY_MODEL_b2c45cc22ce1401b9c5b9b091e41b2c8",
              "IPY_MODEL_c3688bea84e64523ba29a40cb652988d"
            ],
            "layout": "IPY_MODEL_779596defbe14a67b6590aa0d0a8f7c6"
          }
        },
        "9a476f1f88be4cf1b2a66215c74fc4e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86cff63d47724b68aa1f5f48aad4cf5a",
            "placeholder": "​",
            "style": "IPY_MODEL_06c418b4f65448c5941fa97731a1658e",
            "value": "Map: 100%"
          }
        },
        "b2c45cc22ce1401b9c5b9b091e41b2c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa513ff3150a432fbf7da45df0281974",
            "max": 3000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6f9fbefaa5a04fdaae4a16606a31c989",
            "value": 3000
          }
        },
        "c3688bea84e64523ba29a40cb652988d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7763c45f0d8f4fe695d65a99d2a76223",
            "placeholder": "​",
            "style": "IPY_MODEL_e817ac49138247aba853f9158d9e482c",
            "value": " 3000/3000 [05:10&lt;00:00,  9.39 examples/s]"
          }
        },
        "779596defbe14a67b6590aa0d0a8f7c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86cff63d47724b68aa1f5f48aad4cf5a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06c418b4f65448c5941fa97731a1658e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aa513ff3150a432fbf7da45df0281974": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f9fbefaa5a04fdaae4a16606a31c989": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7763c45f0d8f4fe695d65a99d2a76223": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e817ac49138247aba853f9158d9e482c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}